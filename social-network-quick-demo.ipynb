{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Social Science Quick Start: Social Network Analysis\n",
    "\n",
    "**Duration:** 30-45 minutes  \n",
    "**Goal:** Analyze Reddit social network to understand community structure and information diffusion\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "- Load and explore a real social network graph (Reddit comments)\n",
    "- Detect communities using graph algorithms\n",
    "- Identify influential users using centrality measures\n",
    "- Visualize network structure and dynamics\n",
    "- Understand information diffusion patterns\n",
    "\n",
    "## Dataset\n",
    "\n",
    "We'll use a **Reddit Hyperlink Network** dataset:\n",
    "- Nodes: Subreddits (online communities)\n",
    "- Edges: Hyperlinks between subreddits\n",
    "- Attributes: Subreddit properties, post activity\n",
    "- Source: Stanford Large Network Dataset Collection (SNAP)\n",
    "\n",
    "No AWS account or API keys needed - let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries (all pre-installed in Colab/Studio Lab)\n",
    "import warnings\n",
    "from collections import Counter\n",
    "from datetime import datetime\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Set visualization style\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams[\"figure.figsize\"] = (12, 8)\n",
    "plt.rcParams[\"font.size\"] = 10\n",
    "\n",
    "print(\"‚úì Libraries loaded successfully!\")\n",
    "print(f\"Analysis date: {datetime.now().strftime('%Y-%m-%d')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download Reddit hyperlink network from SNAP\n",
    "# This is a curated dataset of subreddit interactions\n",
    "import os\n",
    "import urllib.request\n",
    "\n",
    "# URLs for the dataset\n",
    "edges_url = \"http://snap.stanford.edu/data/soc-redditHyperlinks-body.tsv\"\n",
    "node_props_url = \"http://snap.stanford.edu/data/web-redditEmbeddings-subreddits.csv\"\n",
    "\n",
    "print(\"Downloading Reddit network data...\")\n",
    "print(\"This may take 2-3 minutes (~50MB)\")\n",
    "\n",
    "# Download edge list\n",
    "edges_file = \"reddit_edges.tsv\"\n",
    "if not os.path.exists(edges_file):\n",
    "    urllib.request.urlretrieve(edges_url, edges_file)\n",
    "    print(f\"‚úì Downloaded edge list: {edges_file}\")\n",
    "else:\n",
    "    print(f\"‚úì Using cached edge list: {edges_file}\")\n",
    "\n",
    "# Load the network data\n",
    "edges_df = pd.read_csv(edges_file, sep=\"\\t\")\n",
    "print(f\"\\n‚úì Loaded {len(edges_df):,} hyperlinks between subreddits\")\n",
    "edges_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding the Network\n",
    "\n",
    "Each row represents a **hyperlink** from one subreddit to another:\n",
    "- **SOURCE_SUBREDDIT:** The subreddit posting the link\n",
    "- **TARGET_SUBREDDIT:** The subreddit being linked to\n",
    "- **POST_ID:** The post containing the link\n",
    "- **TIMESTAMP:** When the link was posted\n",
    "- **LINK_SENTIMENT:** Positive or negative sentiment (1 or -1)\n",
    "\n",
    "This creates a **directed network** where edges show information flow between communities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Network Construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build directed network graph\n",
    "print(\"Building network graph...\")\n",
    "\n",
    "G = nx.DiGraph()\n",
    "\n",
    "# Add edges with properties\n",
    "for _, row in edges_df.iterrows():\n",
    "    source = row[\"SOURCE_SUBREDDIT\"]\n",
    "    target = row[\"TARGET_SUBREDDIT\"]\n",
    "    sentiment = row[\"LINK_SENTIMENT\"]\n",
    "\n",
    "    if G.has_edge(source, target):\n",
    "        # Increment weight if edge exists\n",
    "        G[source][target][\"weight\"] += 1\n",
    "    else:\n",
    "        # Add new edge\n",
    "        G.add_edge(source, target, weight=1, sentiment=sentiment)\n",
    "\n",
    "print(\"\\n=== Network Statistics ===\")\n",
    "print(f\"Nodes (subreddits): {G.number_of_nodes():,}\")\n",
    "print(f\"Edges (hyperlinks): {G.number_of_edges():,}\")\n",
    "print(f\"Network density: {nx.density(G):.6f}\")\n",
    "print(f\"Average degree: {sum(dict(G.degree()).values()) / G.number_of_nodes():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate basic network properties\n",
    "print(\"\\n=== Network Connectivity ===\")\n",
    "\n",
    "# Check if network is strongly connected\n",
    "is_strongly_connected = nx.is_strongly_connected(G)\n",
    "print(f\"Strongly connected: {is_strongly_connected}\")\n",
    "\n",
    "# Find largest strongly connected component\n",
    "largest_scc = max(nx.strongly_connected_components(G), key=len)\n",
    "print(\n",
    "    f\"Largest strongly connected component: {len(largest_scc):,} nodes ({len(largest_scc) / G.number_of_nodes() * 100:.1f}%)\"\n",
    ")\n",
    "\n",
    "# Find largest weakly connected component\n",
    "largest_wcc = max(nx.weakly_connected_components(G), key=len)\n",
    "print(\n",
    "    f\"Largest weakly connected component: {len(largest_wcc):,} nodes ({len(largest_wcc) / G.number_of_nodes() * 100:.1f}%)\"\n",
    ")\n",
    "\n",
    "# Work with largest component for analysis\n",
    "G_main = G.subgraph(largest_wcc).copy()\n",
    "print(f\"\\n‚úì Using main component with {G_main.number_of_nodes():,} nodes for analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Degree Distribution Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate degree statistics\n",
    "in_degrees = dict(G_main.in_degree())\n",
    "out_degrees = dict(G_main.out_degree())\n",
    "\n",
    "in_degree_values = list(in_degrees.values())\n",
    "out_degree_values = list(out_degrees.values())\n",
    "\n",
    "print(\"=== Degree Statistics ===\")\n",
    "print(\"\\nIn-degree (links received):\")\n",
    "print(f\"  Mean: {np.mean(in_degree_values):.2f}\")\n",
    "print(f\"  Median: {np.median(in_degree_values):.2f}\")\n",
    "print(f\"  Max: {np.max(in_degree_values)}\")\n",
    "\n",
    "print(\"\\nOut-degree (links posted):\")\n",
    "print(f\"  Mean: {np.mean(out_degree_values):.2f}\")\n",
    "print(f\"  Median: {np.median(out_degree_values):.2f}\")\n",
    "print(f\"  Max: {np.max(out_degree_values)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize degree distributions\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# In-degree distribution (log scale)\n",
    "in_degree_counts = Counter(in_degree_values)\n",
    "degrees, counts = zip(*sorted(in_degree_counts.items()))\n",
    "axes[0].loglog(degrees, counts, \"bo-\", alpha=0.6, markersize=4)\n",
    "axes[0].set_xlabel(\"In-Degree (links received)\", fontsize=12, fontweight=\"bold\")\n",
    "axes[0].set_ylabel(\"Number of Subreddits\", fontsize=12, fontweight=\"bold\")\n",
    "axes[0].set_title(\"In-Degree Distribution (Log-Log)\", fontsize=13, fontweight=\"bold\")\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Out-degree distribution (log scale)\n",
    "out_degree_counts = Counter(out_degree_values)\n",
    "degrees, counts = zip(*sorted(out_degree_counts.items()))\n",
    "axes[1].loglog(degrees, counts, \"ro-\", alpha=0.6, markersize=4)\n",
    "axes[1].set_xlabel(\"Out-Degree (links posted)\", fontsize=12, fontweight=\"bold\")\n",
    "axes[1].set_ylabel(\"Number of Subreddits\", fontsize=12, fontweight=\"bold\")\n",
    "axes[1].set_title(\"Out-Degree Distribution (Log-Log)\", fontsize=13, fontweight=\"bold\")\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüìä Log-log plot shows power-law distribution - characteristic of scale-free networks\")\n",
    "print(\"   Most subreddits have few connections, but some 'hubs' have many connections\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Influence Analysis - Centrality Measures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate various centrality measures\n",
    "print(\"Calculating centrality measures...\")\n",
    "print(\"This may take 2-3 minutes for large networks...\\n\")\n",
    "\n",
    "# PageRank - measures importance based on incoming links\n",
    "pagerank = nx.pagerank(G_main, alpha=0.85)\n",
    "\n",
    "# Betweenness centrality - measures bridging between communities\n",
    "# Use sampling for large networks to speed up computation\n",
    "betweenness = nx.betweenness_centrality(G_main, k=min(1000, G_main.number_of_nodes()))\n",
    "\n",
    "# Degree centrality - simple measure based on number of connections\n",
    "in_degree_centrality = nx.in_degree_centrality(G_main)\n",
    "out_degree_centrality = nx.out_degree_centrality(G_main)\n",
    "\n",
    "print(\"‚úì Centrality measures calculated\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find top influential subreddits by different metrics\n",
    "print(\"=== Top 10 Most Influential Subreddits ===\")\n",
    "\n",
    "print(\"\\nüìç By PageRank (overall importance):\")\n",
    "top_pagerank = sorted(pagerank.items(), key=lambda x: x[1], reverse=True)[:10]\n",
    "for i, (subreddit, score) in enumerate(top_pagerank, 1):\n",
    "    print(f\"  {i}. r/{subreddit}: {score:.6f}\")\n",
    "\n",
    "print(\"\\nüåâ By Betweenness Centrality (bridge communities):\")\n",
    "top_betweenness = sorted(betweenness.items(), key=lambda x: x[1], reverse=True)[:10]\n",
    "for i, (subreddit, score) in enumerate(top_betweenness, 1):\n",
    "    print(f\"  {i}. r/{subreddit}: {score:.6f}\")\n",
    "\n",
    "print(\"\\nüì• By In-Degree (most linked to):\")\n",
    "top_in_degree = sorted(in_degrees.items(), key=lambda x: x[1], reverse=True)[:10]\n",
    "for i, (subreddit, degree) in enumerate(top_in_degree, 1):\n",
    "    print(f\"  {i}. r/{subreddit}: {degree} incoming links\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Community Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to undirected for community detection\n",
    "G_undirected = G_main.to_undirected()\n",
    "\n",
    "print(\"Detecting communities using Louvain algorithm...\")\n",
    "print(\"This may take 1-2 minutes...\\n\")\n",
    "\n",
    "# Use Louvain community detection\n",
    "from networkx.algorithms import community\n",
    "\n",
    "communities = community.greedy_modularity_communities(G_undirected)\n",
    "\n",
    "print(f\"‚úì Detected {len(communities)} communities\")\n",
    "print(\"\\n=== Community Sizes ===\")\n",
    "\n",
    "# Sort communities by size\n",
    "sorted_communities = sorted(communities, key=len, reverse=True)\n",
    "\n",
    "for i, comm in enumerate(sorted_communities[:10], 1):\n",
    "    print(f\"Community {i}: {len(comm):,} subreddits\")\n",
    "\n",
    "# Calculate modularity\n",
    "modularity = community.modularity(G_undirected, communities)\n",
    "print(f\"\\nModularity: {modularity:.4f}\")\n",
    "print(\"(Higher modularity = stronger community structure)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze largest communities\n",
    "print(\"\\n=== Sample Subreddits from Top Communities ===\")\n",
    "\n",
    "for i, comm in enumerate(sorted_communities[:5], 1):\n",
    "    print(f\"\\nCommunity {i} ({len(comm)} subreddits):\")\n",
    "    # Show top 10 most influential nodes in this community\n",
    "    comm_pagerank = {node: pagerank[node] for node in comm if node in pagerank}\n",
    "    top_in_comm = sorted(comm_pagerank.items(), key=lambda x: x[1], reverse=True)[:10]\n",
    "    print(\"  Top subreddits: \" + \", \".join([f\"r/{node}\" for node, _ in top_in_comm]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Network Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create visualization of network structure\n",
    "# Use a subset for visualization (full network too large)\n",
    "print(\"Creating network visualization...\")\n",
    "\n",
    "# Select top nodes by PageRank for visualization\n",
    "top_nodes = sorted(pagerank.items(), key=lambda x: x[1], reverse=True)[:100]\n",
    "top_node_names = [node for node, _ in top_nodes]\n",
    "\n",
    "# Create subgraph\n",
    "G_viz = G_main.subgraph(top_node_names).copy()\n",
    "\n",
    "# Assign communities to nodes\n",
    "node_communities = {}\n",
    "for i, comm in enumerate(sorted_communities):\n",
    "    for node in comm:\n",
    "        node_communities[node] = i\n",
    "\n",
    "# Create color map\n",
    "node_colors = [node_communities.get(node, -1) for node in G_viz.nodes()]\n",
    "node_sizes = [pagerank[node] * 50000 for node in G_viz.nodes()]\n",
    "\n",
    "# Draw network\n",
    "plt.figure(figsize=(16, 12))\n",
    "pos = nx.spring_layout(G_viz, k=0.5, iterations=50, seed=42)\n",
    "\n",
    "nx.draw_networkx_nodes(\n",
    "    G_viz,\n",
    "    pos,\n",
    "    node_color=node_colors,\n",
    "    node_size=node_sizes,\n",
    "    cmap=\"tab20\",\n",
    "    alpha=0.7,\n",
    "    edgecolors=\"black\",\n",
    "    linewidths=0.5,\n",
    ")\n",
    "\n",
    "nx.draw_networkx_edges(\n",
    "    G_viz, pos, alpha=0.2, arrows=True, arrowsize=10, edge_color=\"gray\", width=0.5\n",
    ")\n",
    "\n",
    "# Label top 20 nodes\n",
    "top_20_nodes = top_node_names[:20]\n",
    "labels = {node: f\"r/{node}\" for node in top_20_nodes}\n",
    "nx.draw_networkx_labels(G_viz, pos, labels, font_size=8, font_weight=\"bold\")\n",
    "\n",
    "plt.title(\n",
    "    \"Reddit Subreddit Network - Top 100 by PageRank\\nNode size = influence, Color = community\",\n",
    "    fontsize=15,\n",
    "    fontweight=\"bold\",\n",
    "    pad=20,\n",
    ")\n",
    "plt.axis(\"off\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüìä Visualization shows network structure with:\")\n",
    "print(\"   ‚Ä¢ Node size proportional to PageRank (influence)\")\n",
    "print(\"   ‚Ä¢ Node color represents detected community\")\n",
    "print(\"   ‚Ä¢ Arrows show direction of hyperlinks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Information Diffusion Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze information spread potential\n",
    "print(\"=== Information Diffusion Potential ===\")\n",
    "\n",
    "# Calculate clustering coefficient (how connected neighbors are)\n",
    "clustering = nx.clustering(G_undirected)\n",
    "avg_clustering = np.mean(list(clustering.values()))\n",
    "\n",
    "print(f\"\\nAverage clustering coefficient: {avg_clustering:.4f}\")\n",
    "print(\"(Higher = more tightly clustered communities)\")\n",
    "\n",
    "# Calculate average shortest path length for sample\n",
    "# Use largest connected component\n",
    "largest_cc = max(nx.connected_components(G_undirected), key=len)\n",
    "G_cc = G_undirected.subgraph(largest_cc)\n",
    "\n",
    "# Sample nodes for path length calculation\n",
    "sample_size = min(1000, len(G_cc.nodes()))\n",
    "sample_nodes = np.random.choice(list(G_cc.nodes()), size=sample_size, replace=False)\n",
    "G_sample = G_cc.subgraph(sample_nodes)\n",
    "\n",
    "avg_path_length = nx.average_shortest_path_length(G_sample)\n",
    "print(f\"\\nAverage shortest path length: {avg_path_length:.2f} hops\")\n",
    "print(\"(Information spreads this many steps on average)\")\n",
    "\n",
    "# Calculate diameter\n",
    "diameter = nx.diameter(G_sample)\n",
    "print(f\"Network diameter: {diameter} hops\")\n",
    "print(\"(Maximum distance between any two subreddits)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify key information spreaders\n",
    "print(\"\\n=== Top Information Spreaders ===\")\n",
    "print(\"(Subreddits that bridge multiple communities)\\n\")\n",
    "\n",
    "# Combine metrics to identify super-spreaders\n",
    "spreader_scores = {}\n",
    "for node in G_main.nodes():\n",
    "    # Weighted combination of metrics\n",
    "    score = (\n",
    "        0.4 * pagerank.get(node, 0) * 1000  # Influence\n",
    "        + 0.3 * betweenness.get(node, 0) * 100  # Bridging\n",
    "        + 0.3 * out_degree_centrality.get(node, 0) * 10  # Activity\n",
    "    )\n",
    "    spreader_scores[node] = score\n",
    "\n",
    "top_spreaders = sorted(spreader_scores.items(), key=lambda x: x[1], reverse=True)[:15]\n",
    "\n",
    "for i, (subreddit, score) in enumerate(top_spreaders, 1):\n",
    "    in_deg = in_degrees.get(subreddit, 0)\n",
    "    out_deg = out_degrees.get(subreddit, 0)\n",
    "    pr = pagerank.get(subreddit, 0)\n",
    "    print(\n",
    "        f\"{i:2d}. r/{subreddit:30s} | Score: {score:6.2f} | In: {in_deg:4d} | Out: {out_deg:4d} | PR: {pr:.5f}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Summary and Key Findings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate summary report\n",
    "print(\"=\" * 70)\n",
    "print(\"SOCIAL NETWORK ANALYSIS SUMMARY\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(\"\\nüìä NETWORK STRUCTURE:\")\n",
    "print(f\"   ‚Ä¢ Total subreddits (nodes): {G_main.number_of_nodes():,}\")\n",
    "print(f\"   ‚Ä¢ Total hyperlinks (edges): {G_main.number_of_edges():,}\")\n",
    "print(\n",
    "    f\"   ‚Ä¢ Average connections per subreddit: {sum(dict(G_main.degree()).values()) / G_main.number_of_nodes():.1f}\"\n",
    ")\n",
    "print(f\"   ‚Ä¢ Network density: {nx.density(G_main):.6f}\")\n",
    "\n",
    "print(\"\\nüîç COMMUNITY STRUCTURE:\")\n",
    "print(f\"   ‚Ä¢ Detected communities: {len(communities)}\")\n",
    "print(f\"   ‚Ä¢ Modularity score: {modularity:.4f}\")\n",
    "print(f\"   ‚Ä¢ Largest community: {len(sorted_communities[0]):,} subreddits\")\n",
    "print(f\"   ‚Ä¢ Average clustering: {avg_clustering:.4f}\")\n",
    "\n",
    "print(\"\\nüìà INFLUENCE & DIFFUSION:\")\n",
    "print(f\"   ‚Ä¢ Top influencer: r/{top_pagerank[0][0]} (PageRank: {top_pagerank[0][1]:.6f})\")\n",
    "print(f\"   ‚Ä¢ Top bridge: r/{top_betweenness[0][0]} (Betweenness: {top_betweenness[0][1]:.6f})\")\n",
    "print(f\"   ‚Ä¢ Average path length: {avg_path_length:.2f} hops\")\n",
    "print(f\"   ‚Ä¢ Network diameter: {diameter} hops\")\n",
    "\n",
    "print(\"\\n‚ö° KEY INSIGHTS:\")\n",
    "print(\"   ‚Ä¢ Network exhibits scale-free properties (power-law degree distribution)\")\n",
    "print(f\"   ‚Ä¢ Small-world effect: information spreads quickly (~{avg_path_length:.0f} hops average)\")\n",
    "print(f\"   ‚Ä¢ Strong community structure detected (modularity = {modularity:.2f})\")\n",
    "print(\"   ‚Ä¢ Few highly influential 'hub' subreddits control information flow\")\n",
    "\n",
    "print(\"\\n‚úÖ CONCLUSION:\")\n",
    "print(\"   The Reddit network shows typical social network characteristics: scale-free\")\n",
    "print(\"   topology, strong community clustering, and efficient information diffusion.\")\n",
    "print(\"   Targeting top influencers could amplify message reach by orders of magnitude.\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What You Learned\n",
    "\n",
    "In 30-45 minutes, you:\n",
    "\n",
    "1. ‚úÖ Loaded and analyzed a real-world social network (50K+ nodes)\n",
    "2. ‚úÖ Calculated influence metrics (PageRank, centrality measures)\n",
    "3. ‚úÖ Detected communities using graph algorithms\n",
    "4. ‚úÖ Visualized network structure and dynamics\n",
    "5. ‚úÖ Analyzed information diffusion patterns\n",
    "6. ‚úÖ Identified key influencers and information spreaders\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "### Ready for More?\n",
    "\n",
    "**Tier 1: SageMaker Studio Lab (4-8 hours, free)**\n",
    "- Analyze multiple platforms (Twitter, Reddit, Facebook)\n",
    "- Train Graph Neural Networks for influence prediction\n",
    "- Temporal dynamics analysis (how networks evolve)\n",
    "- Persistent storage for 10GB+ datasets\n",
    "\n",
    "**Tier 2: AWS Starter (2-4 hours, $5-15)**\n",
    "- Store graphs in Neptune (managed graph database)\n",
    "- Real-time influence tracking\n",
    "- Automated analysis with Lambda\n",
    "\n",
    "**Tier 3: Production Infrastructure (4-5 days, $50-500/month)**\n",
    "- Multi-platform integration (10+ social networks)\n",
    "- Streaming data ingestion and analysis\n",
    "- Distributed graph processing with Neptune\n",
    "- AI-powered insights with Amazon Bedrock\n",
    "\n",
    "## Learn More\n",
    "\n",
    "- **Dataset:** [Stanford Network Analysis Project (SNAP)](http://snap.stanford.edu/data/)\n",
    "- **NetworkX Documentation:** [networkx.org](https://networkx.org/)\n",
    "- **Graph Neural Networks:** [PyTorch Geometric](https://pytorch-geometric.readthedocs.io/)\n",
    "\n",
    "---\n",
    "\n",
    "**Generated with [Claude Code](https://claude.com/claude-code)**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
